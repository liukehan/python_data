{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='neural_network.jpg', width=500, height=300>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src='neural_network.jpg', width=500, height=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "        \n",
    "    # Apply the logistic function (sigmoid) to activation_function the output of the neuron\n",
    "    def activation_function(self):#, inputs, weights):\n",
    "        return 1/(1 + math.exp(-(np.dot(self.inputs, self.weights) + self.bias)))\n",
    "    \n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        #self.weights = weights\n",
    "        self.output = self.activation_function()\n",
    "        return self.output\n",
    "                  \n",
    "    # The error for each neuron is calculated by the Mean Square Error method:\n",
    "    def calculate_error(self, target_output):\n",
    "        return 0.5*(target_output - self.output) ** 2\n",
    "                  \n",
    "    # The partial derivate of the error with respect to actual output then is calculated by:\n",
    "    # = 2 * 0.5 * (target output - actual output) ^ (2 - 1) * -1\n",
    "    # = ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
    "    def calculate_partial_derivative_E_yj(self, target_output):\n",
    "        return -(target_output - self.output)\n",
    "    \n",
    "    # The total net input into the neuron is activation_functioned using logistic function to calculate the neuron's output:\n",
    "    # The derivative (not partial derivative since there is only one variable) of the output then is:\n",
    "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ) -- the derivative of sigmoid function\n",
    "    def calculate_partial_derivative_Yj_zj(self):\n",
    "        return self.output*(1 - self.output)\n",
    "                  \n",
    "    # the partial derivative of the error with respect to the total net input.\n",
    "    # This value is also known as the delta (δ) [1]\n",
    "    # δ = ∂E/∂zⱼ = ∂E/∂yⱼ * ∂yⱼ/∂zⱼ \n",
    "    def calculate_partial_derivative_E_zj(self, target_output):\n",
    "        return self.calculate_partial_derivative_E_yj(target_output)*self.calculate_partial_derivative_Yj_zj()\n",
    "\n",
    "    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n",
    "    # = zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n",
    "    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n",
    "    def calculate_partial_derivative_Zj_wi(self, index):\n",
    "        return self.inputs[index]\n",
    "    \n",
    "    # Calculate derivative of Yj to wi\n",
    "    def calculate_partial_derivative_Yj_wi(self, index):\n",
    "        return self.calculate_partial_derivative_Yj_zj()*self.inputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "\n",
    "    def __init__(self, num_neurons, bias):\n",
    "\n",
    "        # Every neuron in a layer shares the same bias-- because it's just for initialization \n",
    "        self.bias = bias if bias else random.random()\n",
    "        random.random()\n",
    "        self.neurons = []\n",
    "        for i in range(num_neurons):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "        \n",
    "    def inspect(self):\n",
    "        # print out info\n",
    "        print('Neurons:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(' Neuron', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        # calculate using inputs and weights\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights=None, hidden_layer_bias=None,\n",
    "                 output_layer_weights=None, output_layer_bias=None, LEARNING_RATE = 0.5):\n",
    "        \n",
    "        # Initialization of a 3 layer network\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layer = NeuronLayer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = NeuronLayer(num_outputs, output_layer_bias)\n",
    "        self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)\n",
    "        self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "\n",
    "    def init_weights_from_inputs_to_hidden_layer_neurons(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)): \n",
    "            for i in range(self.num_inputs):  \n",
    "                if not hidden_layer_weights:\n",
    "                    # if no values for hidden_layer_weights is input, set random values\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for o in range(len(self.output_layer.neurons)):  \n",
    "            for h in range(len(self.hidden_layer.neurons)): \n",
    "                if not output_layer_weights:\n",
    "                    self.output_layer.neurons[o].weights.append(random.random())\n",
    "                else:\n",
    "                    self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])\n",
    "                weight_num += 1\n",
    "\n",
    "    def prinT_NN(self):  # output info for NN\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.prinT_NN()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.prinT_NN()\n",
    "        print('------')\n",
    "\n",
    "    def feed_forward(self, inputs): \n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs) #inputs to hidden\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs) #hidden to outputs\n",
    "\n",
    "    # Uses online learning to train NN\n",
    "    def train(self, training_inputs, training_outputs):\n",
    "        self.feed_forward(training_inputs)\n",
    "        \n",
    "        # 1. Output neuron deltas \n",
    "        partial_derivative_output_neuron_E_zj = [0]*len(self.output_layer.neurons)\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            # ∂E/∂zⱼ=∂E/∂a*∂a/∂z=cost'(target_output)*sigma'(z)\n",
    "            partial_derivative_output_neuron_E_zj[o] = self.output_layer.neurons[\n",
    "                o].calculate_partial_derivative_E_zj(training_outputs[o])\n",
    "\n",
    "        # 2. Hidden neuron deltas\n",
    "        derivative_hidden_neuron_E_yj = [0]*len(self.hidden_layer.neurons)\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            # We need to calculate the derivative of the error with respect to the output of each hidden layer neuron\n",
    "            # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ , y is hidden layer output, \n",
    "            #wij is the weight from hidden layer to output layer\n",
    "            derivative_E_yj = 0\n",
    "            for o in range(len(self.output_layer.neurons)):\n",
    "                derivative_E_yj += partial_derivative_output_neuron_E_zj[o]* \\\n",
    "                                                    self.output_layer.neurons[o].weights[h]\n",
    "            # ∂E/∂zⱼ = dE/dyⱼ * ∂yj/∂zj\n",
    "            derivative_hidden_neuron_E_yj[h] = derivative_E_yj*self.hidden_layer.neurons[\n",
    "                h].calculate_partial_derivative_Yj_zj()\n",
    "\n",
    "        # 3. Update output neuron weights \n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for weight_hidden_output in range(len(self.output_layer.neurons[o].weights)):\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                partial_derivative_E_wi = partial_derivative_output_neuron_E_zj[o]*self.output_layer.neurons[\n",
    "                    o].calculate_partial_derivative_Yj_wi(weight_hidden_output)\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[weight_hidden_output] -= self.LEARNING_RATE*partial_derivative_E_wi\n",
    "\n",
    "        # 4. Update hidden neuron weights \n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for weight_input_hidden in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                partial_derivative_E_wi = derivative_hidden_neuron_E_yj[h]*self.hidden_layer.neurons[\n",
    "                    h].calculate_partial_derivative_Yj_wi(weight_input_hidden)\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.hidden_layer.neurons[h].weights[weight_input_hidden] -= self.LEARNING_RATE*partial_derivative_E_wi\n",
    "\n",
    "    def calculate_total_error(self, training_sets):\n",
    "        total_error = 0\n",
    "        for t in range(len(training_sets)):\n",
    "            training_inputs, training_outputs = training_sets[t]\n",
    "            self.feed_forward(training_inputs)\n",
    "            total_error = sum(self.output_layer.neurons[o].calculate_error(training_outputs[o]) for o in range(len(training_outputs)))\n",
    "        return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.296959421\n",
      "1 0.295535007\n",
      "2 0.294097817\n",
      "3 0.292647808\n",
      "4 0.291184945\n",
      "5 0.2897092\n",
      "6 0.288220552\n",
      "7 0.286718989\n",
      "8 0.285204505\n",
      "9 0.283677107\n",
      "10 0.282136807\n",
      "11 0.28058363\n",
      "12 0.279017608\n",
      "13 0.277438786\n",
      "14 0.275847218\n",
      "15 0.27424297\n",
      "16 0.272626119\n",
      "17 0.270996754\n",
      "18 0.269354976\n",
      "19 0.267700901\n",
      "20 0.266034654\n",
      "21 0.264356378\n",
      "22 0.262666226\n",
      "23 0.260964366\n",
      "24 0.259250982\n",
      "25 0.257526272\n",
      "26 0.255790447\n",
      "27 0.254043737\n",
      "28 0.252286383\n",
      "29 0.250518646\n",
      "30 0.248740799\n",
      "31 0.246953134\n",
      "32 0.245155957\n",
      "33 0.243349591\n",
      "34 0.241534375\n",
      "35 0.239710665\n",
      "36 0.237878831\n",
      "37 0.236039261\n",
      "38 0.234192357\n",
      "39 0.232338538\n",
      "40 0.230478237\n",
      "41 0.228611904\n",
      "42 0.226740002\n",
      "43 0.224863008\n",
      "44 0.222981413\n",
      "45 0.221095723\n",
      "46 0.219206453\n",
      "47 0.217314134\n",
      "48 0.215419304\n",
      "49 0.213522516\n",
      "50 0.211624328\n",
      "51 0.209725312\n",
      "52 0.207826043\n",
      "53 0.205927106\n",
      "54 0.20402909\n",
      "55 0.202132592\n",
      "56 0.20023821\n",
      "57 0.198346546\n",
      "58 0.196458204\n",
      "59 0.194573787\n",
      "60 0.1926939\n",
      "61 0.190819144\n",
      "62 0.188950117\n",
      "63 0.187087416\n",
      "64 0.185231629\n",
      "65 0.183383339\n",
      "66 0.181543123\n",
      "67 0.179711547\n",
      "68 0.177889169\n",
      "69 0.176076534\n",
      "70 0.174274178\n",
      "71 0.172482623\n",
      "72 0.170702376\n",
      "73 0.168933931\n",
      "74 0.167177766\n",
      "75 0.165434344\n",
      "76 0.163704109\n",
      "77 0.161987488\n",
      "78 0.160284892\n",
      "79 0.15859671\n",
      "80 0.156923315\n",
      "81 0.155265058\n",
      "82 0.153622272\n",
      "83 0.15199527\n",
      "84 0.150384343\n",
      "85 0.148789764\n",
      "86 0.147211783\n",
      "87 0.145650633\n",
      "88 0.144106525\n",
      "89 0.142579648\n",
      "90 0.141070175\n",
      "91 0.139578257\n",
      "92 0.138104025\n",
      "93 0.136647592\n",
      "94 0.135209054\n",
      "95 0.133788487\n",
      "96 0.132385948\n",
      "97 0.131001479\n",
      "98 0.129635104\n",
      "99 0.128286833\n",
      "100 0.126956657\n",
      "101 0.125644555\n",
      "102 0.124350489\n",
      "103 0.12307441\n",
      "104 0.121816253\n",
      "105 0.120575943\n",
      "106 0.119353392\n",
      "107 0.1181485\n",
      "108 0.116961157\n",
      "109 0.115791244\n",
      "110 0.11463863\n",
      "111 0.113503177\n",
      "112 0.11238474\n",
      "113 0.111283164\n",
      "114 0.110198288\n",
      "115 0.109129944\n",
      "116 0.108077959\n",
      "117 0.107042153\n",
      "118 0.106022344\n",
      "119 0.105018342\n",
      "120 0.104029955\n",
      "121 0.103056988\n",
      "122 0.102099242\n",
      "123 0.101156515\n",
      "124 0.100228603\n",
      "125 0.099315302\n",
      "126 0.098416403\n",
      "127 0.097531698\n",
      "128 0.096660979\n",
      "129 0.095804036\n",
      "130 0.094960658\n",
      "131 0.094130636\n",
      "132 0.09331376\n",
      "133 0.092509821\n",
      "134 0.091718611\n",
      "135 0.090939921\n",
      "136 0.090173546\n",
      "137 0.089419281\n",
      "138 0.088676921\n",
      "139 0.087946264\n",
      "140 0.087227111\n",
      "141 0.086519263\n",
      "142 0.085822524\n",
      "143 0.085136698\n",
      "144 0.084461594\n",
      "145 0.083797022\n",
      "146 0.083142794\n",
      "147 0.082498725\n",
      "148 0.081864632\n",
      "149 0.081240335\n",
      "150 0.080625657\n",
      "151 0.080020421\n",
      "152 0.079424457\n",
      "153 0.078837593\n",
      "154 0.078259662\n",
      "155 0.077690501\n",
      "156 0.077129947\n",
      "157 0.076577841\n",
      "158 0.076034027\n",
      "159 0.075498351\n",
      "160 0.074970661\n",
      "161 0.07445081\n",
      "162 0.073938651\n",
      "163 0.073434041\n",
      "164 0.07293684\n",
      "165 0.07244691\n",
      "166 0.071964115\n",
      "167 0.071488322\n",
      "168 0.071019402\n",
      "169 0.070557226\n",
      "170 0.070101668\n",
      "171 0.069652607\n",
      "172 0.06920992\n",
      "173 0.06877349\n",
      "174 0.068343202\n",
      "175 0.067918941\n",
      "176 0.067500595\n",
      "177 0.067088057\n",
      "178 0.066681218\n",
      "179 0.066279975\n",
      "180 0.065884223\n",
      "181 0.065493864\n",
      "182 0.065108797\n",
      "183 0.064728927\n",
      "184 0.064354159\n",
      "185 0.0639844\n",
      "186 0.06361956\n",
      "187 0.063259549\n",
      "188 0.06290428\n",
      "189 0.062553668\n",
      "190 0.06220763\n",
      "191 0.061866083\n",
      "192 0.061528947\n",
      "193 0.061196144\n",
      "194 0.060867597\n",
      "195 0.060543231\n",
      "196 0.060222971\n",
      "197 0.059906746\n",
      "198 0.059594485\n",
      "199 0.059286118\n",
      "200 0.058981578\n",
      "201 0.058680798\n",
      "202 0.058383712\n",
      "203 0.058090258\n",
      "204 0.057800372\n",
      "205 0.057513994\n",
      "206 0.057231063\n",
      "207 0.056951521\n",
      "208 0.05667531\n",
      "209 0.056402374\n",
      "210 0.056132658\n",
      "211 0.055866106\n",
      "212 0.055602668\n",
      "213 0.05534229\n",
      "214 0.055084921\n",
      "215 0.054830513\n",
      "216 0.054579015\n",
      "217 0.054330381\n",
      "218 0.054084562\n",
      "219 0.053841514\n",
      "220 0.053601191\n",
      "221 0.05336355\n",
      "222 0.053128545\n",
      "223 0.052896137\n",
      "224 0.052666282\n",
      "225 0.05243894\n",
      "226 0.05221407\n",
      "227 0.051991635\n",
      "228 0.051771596\n",
      "229 0.051553914\n",
      "230 0.051338553\n",
      "231 0.051125477\n",
      "232 0.050914649\n",
      "233 0.050706037\n",
      "234 0.050499604\n",
      "235 0.050295319\n",
      "236 0.050093147\n",
      "237 0.049893057\n",
      "238 0.049695017\n",
      "239 0.049498997\n",
      "240 0.049304965\n",
      "241 0.049112892\n",
      "242 0.048922748\n",
      "243 0.048734506\n",
      "244 0.048548136\n",
      "245 0.048363611\n",
      "246 0.048180904\n",
      "247 0.047999989\n",
      "248 0.047820839\n",
      "249 0.047643428\n",
      "250 0.047467732\n",
      "251 0.047293725\n",
      "252 0.047121383\n",
      "253 0.046950684\n",
      "254 0.046781602\n",
      "255 0.046614115\n",
      "256 0.046448201\n",
      "257 0.046283838\n",
      "258 0.046121003\n",
      "259 0.045959676\n",
      "260 0.045799835\n",
      "261 0.04564146\n",
      "262 0.045484531\n",
      "263 0.045329027\n",
      "264 0.045174929\n",
      "265 0.045022218\n",
      "266 0.044870876\n",
      "267 0.044720883\n",
      "268 0.044572221\n",
      "269 0.044424873\n",
      "270 0.044278821\n",
      "271 0.044134048\n",
      "272 0.043990536\n",
      "273 0.04384827\n",
      "274 0.043707232\n",
      "275 0.043567406\n",
      "276 0.043428777\n",
      "277 0.043291329\n",
      "278 0.043155047\n",
      "279 0.043019916\n",
      "280 0.042885921\n",
      "281 0.042753047\n",
      "282 0.04262128\n",
      "283 0.042490606\n",
      "284 0.042361011\n",
      "285 0.042232482\n",
      "286 0.042105005\n",
      "287 0.041978566\n",
      "288 0.041853154\n",
      "289 0.041728755\n",
      "290 0.041605356\n",
      "291 0.041482946\n",
      "292 0.041361512\n",
      "293 0.041241042\n",
      "294 0.041121525\n",
      "295 0.041002948\n",
      "296 0.040885301\n",
      "297 0.040768573\n",
      "298 0.040652751\n",
      "299 0.040537826\n",
      "300 0.040423787\n",
      "301 0.040310623\n",
      "302 0.040198324\n",
      "303 0.040086879\n",
      "304 0.039976279\n",
      "305 0.039866514\n",
      "306 0.039757574\n",
      "307 0.039649449\n",
      "308 0.039542131\n",
      "309 0.039435609\n",
      "310 0.039329874\n",
      "311 0.039224918\n",
      "312 0.039120732\n",
      "313 0.039017306\n",
      "314 0.038914633\n",
      "315 0.038812704\n",
      "316 0.03871151\n",
      "317 0.038611043\n",
      "318 0.038511296\n",
      "319 0.038412259\n",
      "320 0.038313926\n",
      "321 0.038216288\n",
      "322 0.038119339\n",
      "323 0.038023069\n",
      "324 0.037927472\n",
      "325 0.037832541\n",
      "326 0.037738268\n",
      "327 0.037644646\n",
      "328 0.037551669\n",
      "329 0.037459328\n",
      "330 0.037367619\n",
      "331 0.037276533\n",
      "332 0.037186064\n",
      "333 0.037096206\n",
      "334 0.037006952\n",
      "335 0.036918295\n",
      "336 0.036830231\n",
      "337 0.036742751\n",
      "338 0.036655851\n",
      "339 0.036569525\n",
      "340 0.036483766\n",
      "341 0.036398568\n",
      "342 0.036313926\n",
      "343 0.036229834\n",
      "344 0.036146287\n",
      "345 0.036063279\n",
      "346 0.035980804\n",
      "347 0.035898858\n",
      "348 0.035817434\n",
      "349 0.035736528\n",
      "350 0.035656135\n",
      "351 0.035576249\n",
      "352 0.035496865\n",
      "353 0.035417979\n",
      "354 0.035339585\n",
      "355 0.035261679\n",
      "356 0.035184255\n",
      "357 0.03510731\n",
      "358 0.035030838\n",
      "359 0.034954836\n",
      "360 0.034879297\n",
      "361 0.034804219\n",
      "362 0.034729596\n",
      "363 0.034655425\n",
      "364 0.0345817\n",
      "365 0.034508418\n",
      "366 0.034435575\n",
      "367 0.034363165\n",
      "368 0.034291187\n",
      "369 0.034219634\n",
      "370 0.034148503\n",
      "371 0.034077791\n",
      "372 0.034007493\n",
      "373 0.033937606\n",
      "374 0.033868126\n",
      "375 0.033799048\n",
      "376 0.033730371\n",
      "377 0.033662088\n",
      "378 0.033594198\n",
      "379 0.033526697\n",
      "380 0.03345958\n",
      "381 0.033392845\n",
      "382 0.033326488\n",
      "383 0.033260506\n",
      "384 0.033194896\n",
      "385 0.033129653\n",
      "386 0.033064775\n",
      "387 0.033000259\n",
      "388 0.032936101\n",
      "389 0.032872299\n",
      "390 0.032808848\n",
      "391 0.032745747\n",
      "392 0.032682992\n",
      "393 0.032620579\n",
      "394 0.032558507\n",
      "395 0.032496772\n",
      "396 0.032435371\n",
      "397 0.032374301\n",
      "398 0.032313559\n",
      "399 0.032253144\n",
      "400 0.032193051\n",
      "401 0.032133279\n",
      "402 0.032073823\n",
      "403 0.032014683\n",
      "404 0.031955855\n",
      "405 0.031897336\n",
      "406 0.031839124\n",
      "407 0.031781217\n",
      "408 0.031723612\n",
      "409 0.031666305\n",
      "410 0.031609296\n",
      "411 0.031552581\n",
      "412 0.031496158\n",
      "413 0.031440025\n",
      "414 0.031384178\n",
      "415 0.031328617\n",
      "416 0.031273339\n",
      "417 0.03121834\n",
      "418 0.03116362\n",
      "419 0.031109175\n",
      "420 0.031055005\n",
      "421 0.031001105\n",
      "422 0.030947475\n",
      "423 0.030894112\n",
      "424 0.030841013\n",
      "425 0.030788178\n",
      "426 0.030735603\n",
      "427 0.030683288\n",
      "428 0.030631228\n",
      "429 0.030579424\n",
      "430 0.030527872\n",
      "431 0.030476571\n",
      "432 0.030425519\n",
      "433 0.030374714\n",
      "434 0.030324154\n",
      "435 0.030273836\n",
      "436 0.03022376\n",
      "437 0.030173924\n",
      "438 0.030124325\n",
      "439 0.030074961\n",
      "440 0.030025832\n",
      "441 0.029976935\n",
      "442 0.029928268\n",
      "443 0.02987983\n",
      "444 0.029831619\n",
      "445 0.029783633\n",
      "446 0.029735871\n",
      "447 0.029688331\n",
      "448 0.029641011\n",
      "449 0.029593909\n",
      "450 0.029547025\n",
      "451 0.029500356\n",
      "452 0.029453901\n",
      "453 0.029407658\n",
      "454 0.029361626\n",
      "455 0.029315803\n",
      "456 0.029270188\n",
      "457 0.029224779\n",
      "458 0.029179575\n",
      "459 0.029134574\n",
      "460 0.029089774\n",
      "461 0.029045175\n",
      "462 0.029000775\n",
      "463 0.028956572\n",
      "464 0.028912565\n",
      "465 0.028868753\n",
      "466 0.028825134\n",
      "467 0.028781707\n",
      "468 0.028738471\n",
      "469 0.028695424\n",
      "470 0.028652564\n",
      "471 0.028609892\n",
      "472 0.028567404\n",
      "473 0.0285251\n",
      "474 0.028482979\n",
      "475 0.02844104\n",
      "476 0.028399281\n",
      "477 0.0283577\n",
      "478 0.028316298\n",
      "479 0.028275071\n",
      "480 0.028234021\n",
      "481 0.028193144\n",
      "482 0.02815244\n",
      "483 0.028111908\n",
      "484 0.028071546\n",
      "485 0.028031354\n",
      "486 0.02799133\n",
      "487 0.027951473\n",
      "488 0.027911782\n",
      "489 0.027872256\n",
      "490 0.027832894\n",
      "491 0.027793695\n",
      "492 0.027754657\n",
      "493 0.02771578\n",
      "494 0.027677062\n",
      "495 0.027638503\n",
      "496 0.027600101\n",
      "497 0.027561856\n",
      "498 0.027523766\n",
      "499 0.02748583\n",
      "500 0.027448048\n",
      "501 0.027410418\n",
      "502 0.027372939\n",
      "503 0.027335611\n",
      "504 0.027298432\n",
      "505 0.027261401\n",
      "506 0.027224518\n",
      "507 0.027187782\n",
      "508 0.027151191\n",
      "509 0.027114745\n",
      "510 0.027078443\n",
      "511 0.027042283\n",
      "512 0.027006265\n",
      "513 0.026970389\n",
      "514 0.026934653\n",
      "515 0.026899056\n",
      "516 0.026863597\n",
      "517 0.026828276\n",
      "518 0.026793091\n",
      "519 0.026758043\n",
      "520 0.026723129\n",
      "521 0.02668835\n",
      "522 0.026653704\n",
      "523 0.02661919\n",
      "524 0.026584808\n",
      "525 0.026550557\n",
      "526 0.026516436\n",
      "527 0.026482444\n",
      "528 0.026448581\n",
      "529 0.026414845\n",
      "530 0.026381237\n",
      "531 0.026347754\n",
      "532 0.026314397\n",
      "533 0.026281165\n",
      "534 0.026248056\n",
      "535 0.026215071\n",
      "536 0.026182208\n",
      "537 0.026149467\n",
      "538 0.026116846\n",
      "539 0.026084346\n",
      "540 0.026051966\n",
      "541 0.026019704\n",
      "542 0.02598756\n",
      "543 0.025955534\n",
      "544 0.025923624\n",
      "545 0.025891831\n",
      "546 0.025860153\n",
      "547 0.025828589\n",
      "548 0.02579714\n",
      "549 0.025765804\n",
      "550 0.02573458\n",
      "551 0.025703469\n",
      "552 0.025672469\n",
      "553 0.025641579\n",
      "554 0.0256108\n",
      "555 0.02558013\n",
      "556 0.02554957\n",
      "557 0.025519117\n",
      "558 0.025488772\n",
      "559 0.025458534\n",
      "560 0.025428402\n",
      "561 0.025398376\n",
      "562 0.025368455\n",
      "563 0.025338639\n",
      "564 0.025308927\n",
      "565 0.025279318\n",
      "566 0.025249812\n",
      "567 0.025220409\n",
      "568 0.025191107\n",
      "569 0.025161906\n",
      "570 0.025132805\n",
      "571 0.025103805\n",
      "572 0.025074904\n",
      "573 0.025046102\n",
      "574 0.025017398\n",
      "575 0.024988792\n",
      "576 0.024960283\n",
      "577 0.024931871\n",
      "578 0.024903556\n",
      "579 0.024875336\n",
      "580 0.024847211\n",
      "581 0.02481918\n",
      "582 0.024791244\n",
      "583 0.024763401\n",
      "584 0.024735652\n",
      "585 0.024707995\n",
      "586 0.02468043\n",
      "587 0.024652957\n",
      "588 0.024625574\n",
      "589 0.024598283\n",
      "590 0.024571081\n",
      "591 0.02454397\n",
      "592 0.024516947\n",
      "593 0.024490013\n",
      "594 0.024463167\n",
      "595 0.024436409\n",
      "596 0.024409739\n",
      "597 0.024383155\n",
      "598 0.024356658\n",
      "599 0.024330246\n",
      "600 0.02430392\n",
      "601 0.024277679\n",
      "602 0.024251523\n",
      "603 0.02422545\n",
      "604 0.024199462\n",
      "605 0.024173557\n",
      "606 0.024147734\n",
      "607 0.024121994\n",
      "608 0.024096336\n",
      "609 0.02407076\n",
      "610 0.024045264\n",
      "611 0.02401985\n",
      "612 0.023994515\n",
      "613 0.023969261\n",
      "614 0.023944086\n",
      "615 0.02391899\n",
      "616 0.023893973\n",
      "617 0.023869034\n",
      "618 0.023844173\n",
      "619 0.02381939\n",
      "620 0.023794683\n",
      "621 0.023770054\n",
      "622 0.023745501\n",
      "623 0.023721023\n",
      "624 0.023696621\n",
      "625 0.023672295\n",
      "626 0.023648043\n",
      "627 0.023623866\n",
      "628 0.023599763\n",
      "629 0.023575733\n",
      "630 0.023551777\n",
      "631 0.023527894\n",
      "632 0.023504083\n",
      "633 0.023480345\n",
      "634 0.023456678\n",
      "635 0.023433083\n",
      "636 0.02340956\n",
      "637 0.023386107\n",
      "638 0.023362725\n",
      "639 0.023339413\n",
      "640 0.02331617\n",
      "641 0.023292998\n",
      "642 0.023269894\n",
      "643 0.023246859\n",
      "644 0.023223893\n",
      "645 0.023200995\n",
      "646 0.023178164\n",
      "647 0.023155401\n",
      "648 0.023132706\n",
      "649 0.023110077\n",
      "650 0.023087515\n",
      "651 0.023065019\n",
      "652 0.023042588\n",
      "653 0.023020224\n",
      "654 0.022997925\n",
      "655 0.02297569\n",
      "656 0.022953521\n",
      "657 0.022931415\n",
      "658 0.022909374\n",
      "659 0.022887397\n",
      "660 0.022865483\n",
      "661 0.022843632\n",
      "662 0.022821844\n",
      "663 0.022800119\n",
      "664 0.022778456\n",
      "665 0.022756855\n",
      "666 0.022735315\n",
      "667 0.022713837\n",
      "668 0.02269242\n",
      "669 0.022671064\n",
      "670 0.022649769\n",
      "671 0.022628534\n",
      "672 0.022607359\n",
      "673 0.022586244\n",
      "674 0.022565188\n",
      "675 0.022544191\n",
      "676 0.022523253\n",
      "677 0.022502374\n",
      "678 0.022481554\n",
      "679 0.022460791\n",
      "680 0.022440086\n",
      "681 0.022419439\n",
      "682 0.02239885\n",
      "683 0.022378317\n",
      "684 0.022357841\n",
      "685 0.022337422\n",
      "686 0.022317059\n",
      "687 0.022296752\n",
      "688 0.022276501\n",
      "689 0.022256306\n",
      "690 0.022236166\n",
      "691 0.022216081\n",
      "692 0.02219605\n",
      "693 0.022176075\n",
      "694 0.022156153\n",
      "695 0.022136286\n",
      "696 0.022116473\n",
      "697 0.022096713\n",
      "698 0.022077007\n",
      "699 0.022057354\n",
      "700 0.022037754\n",
      "701 0.022018206\n",
      "702 0.021998711\n",
      "703 0.021979269\n",
      "704 0.021959878\n",
      "705 0.021940539\n",
      "706 0.021921251\n",
      "707 0.021902015\n",
      "708 0.02188283\n",
      "709 0.021863696\n",
      "710 0.021844613\n",
      "711 0.02182558\n",
      "712 0.021806597\n",
      "713 0.021787665\n",
      "714 0.021768782\n",
      "715 0.021749949\n",
      "716 0.021731165\n",
      "717 0.02171243\n",
      "718 0.021693745\n",
      "719 0.021675108\n",
      "720 0.021656519\n",
      "721 0.021637979\n",
      "722 0.021619488\n",
      "723 0.021601044\n",
      "724 0.021582647\n",
      "725 0.021564299\n",
      "726 0.021545998\n",
      "727 0.021527743\n",
      "728 0.021509536\n",
      "729 0.021491376\n",
      "730 0.021473262\n",
      "731 0.021455194\n",
      "732 0.021437173\n",
      "733 0.021419198\n",
      "734 0.021401268\n",
      "735 0.021383384\n",
      "736 0.021365545\n",
      "737 0.021347752\n",
      "738 0.021330004\n",
      "739 0.0213123\n",
      "740 0.021294641\n",
      "741 0.021277027\n",
      "742 0.021259457\n",
      "743 0.021241931\n",
      "744 0.021224449\n",
      "745 0.021207011\n",
      "746 0.021189616\n",
      "747 0.021172265\n",
      "748 0.021154957\n",
      "749 0.021137692\n",
      "750 0.02112047\n",
      "751 0.021103291\n",
      "752 0.021086154\n",
      "753 0.021069059\n",
      "754 0.021052007\n",
      "755 0.021034997\n",
      "756 0.021018028\n",
      "757 0.021001102\n",
      "758 0.020984216\n",
      "759 0.020967373\n",
      "760 0.02095057\n",
      "761 0.020933808\n",
      "762 0.020917088\n",
      "763 0.020900408\n",
      "764 0.020883768\n",
      "765 0.020867169\n",
      "766 0.02085061\n",
      "767 0.020834091\n",
      "768 0.020817612\n",
      "769 0.020801173\n",
      "770 0.020784774\n",
      "771 0.020768414\n",
      "772 0.020752093\n",
      "773 0.020735811\n",
      "774 0.020719568\n",
      "775 0.020703364\n",
      "776 0.020687199\n",
      "777 0.020671072\n",
      "778 0.020654984\n",
      "779 0.020638934\n",
      "780 0.020622922\n",
      "781 0.020606948\n",
      "782 0.020591012\n",
      "783 0.020575113\n",
      "784 0.020559252\n",
      "785 0.020543428\n",
      "786 0.020527641\n",
      "787 0.020511892\n",
      "788 0.020496179\n",
      "789 0.020480503\n",
      "790 0.020464864\n",
      "791 0.020449261\n",
      "792 0.020433695\n",
      "793 0.020418165\n",
      "794 0.020402671\n",
      "795 0.020387213\n",
      "796 0.020371791\n",
      "797 0.020356404\n",
      "798 0.020341053\n",
      "799 0.020325738\n",
      "800 0.020310458\n",
      "801 0.020295213\n",
      "802 0.020280002\n",
      "803 0.020264827\n",
      "804 0.020249687\n",
      "805 0.020234581\n",
      "806 0.02021951\n",
      "807 0.020204473\n",
      "808 0.02018947\n",
      "809 0.020174501\n",
      "810 0.020159567\n",
      "811 0.020144666\n",
      "812 0.020129799\n",
      "813 0.020114966\n",
      "814 0.020100166\n",
      "815 0.020085399\n",
      "816 0.020070666\n",
      "817 0.020055966\n",
      "818 0.020041299\n",
      "819 0.020026665\n",
      "820 0.020012063\n",
      "821 0.019997494\n",
      "822 0.019982958\n",
      "823 0.019968454\n",
      "824 0.019953983\n",
      "825 0.019939543\n",
      "826 0.019925136\n",
      "827 0.01991076\n",
      "828 0.019896417\n",
      "829 0.019882105\n",
      "830 0.019867825\n",
      "831 0.019853576\n",
      "832 0.019839358\n",
      "833 0.019825172\n",
      "834 0.019811017\n",
      "835 0.019796893\n",
      "836 0.0197828\n",
      "837 0.019768738\n",
      "838 0.019754706\n",
      "839 0.019740705\n",
      "840 0.019726735\n",
      "841 0.019712794\n",
      "842 0.019698885\n",
      "843 0.019685005\n",
      "844 0.019671155\n",
      "845 0.019657336\n",
      "846 0.019643546\n",
      "847 0.019629786\n",
      "848 0.019616055\n",
      "849 0.019602354\n",
      "850 0.019588683\n",
      "851 0.019575041\n",
      "852 0.019561428\n",
      "853 0.019547844\n",
      "854 0.019534289\n",
      "855 0.019520763\n",
      "856 0.019507266\n",
      "857 0.019493798\n",
      "858 0.019480358\n",
      "859 0.019466947\n",
      "860 0.019453564\n",
      "861 0.019440209\n",
      "862 0.019426883\n",
      "863 0.019413585\n",
      "864 0.019400315\n",
      "865 0.019387073\n",
      "866 0.019373858\n",
      "867 0.019360672\n",
      "868 0.019347513\n",
      "869 0.019334381\n",
      "870 0.019321277\n",
      "871 0.019308201\n",
      "872 0.019295152\n",
      "873 0.019282129\n",
      "874 0.019269134\n",
      "875 0.019256166\n",
      "876 0.019243225\n",
      "877 0.019230311\n",
      "878 0.019217423\n",
      "879 0.019204562\n",
      "880 0.019191728\n",
      "881 0.01917892\n",
      "882 0.019166138\n",
      "883 0.019153383\n",
      "884 0.019140654\n",
      "885 0.019127951\n",
      "886 0.019115274\n",
      "887 0.019102623\n",
      "888 0.019089998\n",
      "889 0.019077398\n",
      "890 0.019064824\n",
      "891 0.019052276\n",
      "892 0.019039753\n",
      "893 0.019027256\n",
      "894 0.019014784\n",
      "895 0.019002337\n",
      "896 0.018989916\n",
      "897 0.018977519\n",
      "898 0.018965148\n",
      "899 0.018952801\n",
      "900 0.018940479\n",
      "901 0.018928182\n",
      "902 0.01891591\n",
      "903 0.018903662\n",
      "904 0.018891439\n",
      "905 0.01887924\n",
      "906 0.018867066\n",
      "907 0.018854915\n",
      "908 0.018842789\n",
      "909 0.018830687\n",
      "910 0.01881861\n",
      "911 0.018806556\n",
      "912 0.018794526\n",
      "913 0.018782519\n",
      "914 0.018770537\n",
      "915 0.018758578\n",
      "916 0.018746643\n",
      "917 0.018734731\n",
      "918 0.018722843\n",
      "919 0.018710978\n",
      "920 0.018699136\n",
      "921 0.018687318\n",
      "922 0.018675522\n",
      "923 0.01866375\n",
      "924 0.018652\n",
      "925 0.018640274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926 0.01862857\n",
      "927 0.018616889\n",
      "928 0.018605231\n",
      "929 0.018593596\n",
      "930 0.018581983\n",
      "931 0.018570392\n",
      "932 0.018558824\n",
      "933 0.018547278\n",
      "934 0.018535754\n",
      "935 0.018524253\n",
      "936 0.018512774\n",
      "937 0.018501316\n",
      "938 0.018489881\n",
      "939 0.018478468\n",
      "940 0.018467076\n",
      "941 0.018455707\n",
      "942 0.018444359\n",
      "943 0.018433032\n",
      "944 0.018421727\n",
      "945 0.018410444\n",
      "946 0.018399182\n",
      "947 0.018387942\n",
      "948 0.018376723\n",
      "949 0.018365525\n",
      "950 0.018354348\n",
      "951 0.018343192\n",
      "952 0.018332058\n",
      "953 0.018320944\n",
      "954 0.018309851\n",
      "955 0.018298779\n",
      "956 0.018287728\n",
      "957 0.018276698\n",
      "958 0.018265688\n",
      "959 0.018254699\n",
      "960 0.018243731\n",
      "961 0.018232782\n",
      "962 0.018221855\n",
      "963 0.018210947\n",
      "964 0.01820006\n",
      "965 0.018189193\n",
      "966 0.018178347\n",
      "967 0.01816752\n",
      "968 0.018156714\n",
      "969 0.018145927\n",
      "970 0.01813516\n",
      "971 0.018124414\n",
      "972 0.018113687\n",
      "973 0.018102979\n",
      "974 0.018092292\n",
      "975 0.018081624\n",
      "976 0.018070975\n",
      "977 0.018060347\n",
      "978 0.018049737\n",
      "979 0.018039147\n",
      "980 0.018028576\n",
      "981 0.018018025\n",
      "982 0.018007493\n",
      "983 0.01799698\n",
      "984 0.017986486\n",
      "985 0.017976011\n",
      "986 0.017965555\n",
      "987 0.017955118\n",
      "988 0.017944699\n",
      "989 0.0179343\n",
      "990 0.01792392\n",
      "991 0.017913558\n",
      "992 0.017903214\n",
      "993 0.01789289\n",
      "994 0.017882584\n",
      "995 0.017872296\n",
      "996 0.017862027\n",
      "997 0.017851776\n",
      "998 0.017841544\n",
      "999 0.01783133\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3], hidden_layer_bias=0.35,\n",
    "                   output_layer_weights=[0.4, 0.45, 0.5, 0.55], output_layer_bias=0.6)\n",
    "for i in range(1000):\n",
    "    nn.train([0.8, 0.2], [0.01, 0.99])\n",
    "    print(i, round(nn.calculate_total_error([[[0.05, 0.1], [0.01, 0.99]]]), 9)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
